{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tf115]","language":"python","name":"conda-env-tf115-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Keras_Retina_Raccoon_학습및_Detection_colab.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"W1VxQUTQ0knM","colab_type":"text"},"source":["### Raccoon 데이터 세트를 학습하고 학습된 모델을 이용하여 이미지와 비디오에 Object Detection과 성능 평가. "]},{"cell_type":"markdown","metadata":{"id":"NhGsWz-t1ZWa","colab_type":"text"},"source":["### 본 실습 예제는 GPU를 활용하므로 상단 메뉴에서 런타임->런타임 유형 변경에서 GPU를 선택해 주십시요.\n","\n","### tensorflow, keras 설치 및 강의 실습코드/데이터 Download\n","\n","Colab 버전은 colab에서 pip 명령어를 이용하여 tensorflow 1.15, keras 2.3를 소스코드 커널 기동시 마다 설치해야 합니다\n","\n","Colab의 tensorflow는 2020년 기준으로 2.2 이며, keras는 2.3입니다. 실습코드는 tensorflow 1.15 기준으로 되어 있으므로 이를 downgrade해야 합니다.\n","\n","pip를 이용하여 tensorflow 1.15을 설치하면 자동으로 downgrade 됩니다.\n","\n","OpenCV는 Colab에서 이미 설치 되어 있으니 추가설치는 필요 없습니다.\n","\n","강의 실습코드와 데이터는 https://github.com/chulminkw/DLCV.git 에서 다운로드 할 수 있습니다. "]},{"cell_type":"code","metadata":{"id":"mFHLxch31k56","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":580},"outputId":"fdbe0254-eb53-4d15-b400-a991c2f9c4a9"},"source":["# 현재 디렉토리는 /content이며 이 디렉토리를 기준으로 실습코드와 데이터를 다운로드 합니다. \n","!pwd\n","!rm -rf DLCV\n","!git clone https://github.com/chulminkw/DLCV.git\n","# DLCV 디렉토리가 Download되고 DLCV 밑에 Detection과 Segmentation 디렉토리가 있는 것을 확인\n","!ls -lia \n","!ls -lia DLCV\n","\n","# tensorflow 1.15을 설치합니다. 자동으로 tensorflow 2.2가 1.15으로 downgrade 됩니다. \n","!pip install tensorflow-gpu==1.15.2 \n","# keras 2.3를 설치합니다. \n","!pip install keras==2.3.0\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'DLCV'...\n","remote: Enumerating objects: 37, done.\u001b[K\n","remote: Counting objects: 100% (37/37), done.\u001b[K\n","remote: Compressing objects: 100% (33/33), done.\u001b[K\n","remote: Total 196 (delta 13), reused 0 (delta 0), pack-reused 159\u001b[K\n","Receiving objects: 100% (196/196), 138.04 MiB | 34.84 MiB/s, done.\n","Resolving deltas: 100% (70/70), done.\n","total 20\n","3538981 drwxr-xr-x 1 root root 4096 Aug 29 12:31 .\n","3291041 drwxr-xr-x 1 root root 4096 Aug 29 12:30 ..\n","3538982 drwxr-xr-x 1 root root 4096 Aug 27 16:39 .config\n","3291129 drwxr-xr-x 7 root root 4096 Aug 29 12:31 DLCV\n","2097207 drwxr-xr-x 1 root root 4096 Aug 27 16:39 sample_data\n","total 5888\n","3291129 drwxr-xr-x 7 root root    4096 Aug 29 12:31 .\n","3538981 drwxr-xr-x 1 root root    4096 Aug 29 12:31 ..\n","3291206 drwxr-xr-x 2 root root    4096 Aug 29 12:31 colab_tf115_modify_files\n","3291209 drwxr-xr-x 6 root root    4096 Aug 29 12:31 data\n","3291172 drwxr-xr-x 8 root root    4096 Aug 29 12:31 Detection\n","3291157 -rw-r--r-- 1 root root 5992976 Aug 29 12:31 DLCV_Colab_SrcCode_new.zip\n","3291130 drwxr-xr-x 8 root root    4096 Aug 29 12:31 .git\n","3291198 -rw-r--r-- 1 root root     142 Aug 29 12:31 README.md\n","3291199 drwxr-xr-x 3 root root    4096 Aug 29 12:31 Segmentation\n","Collecting tensorflow-gpu==1.15.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/ca/58e40e5077fa2a92004f398d705a288e958434f123938f4ce75ffe25b64b/tensorflow_gpu-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (411.0MB)\n","\u001b[K     |████████████████████████████████| 411.0MB 40kB/s \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tDO9ihn46ufL","colab_type":"code","colab":{}},"source":["# tensorflow는 1.15, keras는 2.3 버전 확인\n","import tensorflow as tf\n","import keras\n","\n","print(tf.__version__)\n","print(keras.__version__)\n","\n","# gpu가 세팅되어 있는지 확인. \n","tf.test.gpu_device_name()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DXxnk3bkKhCU","colab_type":"text"},"source":["#### keras-retinanet 다운로드 및 설치\n","* fizyr keras-retinanet이 현재 keras 2.4 로 마이그레이션 되면서 버그가 많아짐.\n","* tensorflow 1.15와 호환되는 keras-retinanet 버전(v0.5.1) 다운로드를 https://github.com/chulminkw/keras-retinanet-tf115.git 에서 수행.\n","* /content/DLCV/Detection/retina/keras-retinanet 디렉토리에 download 되고 설치 됩니다. \n"]},{"cell_type":"code","metadata":{"id":"Oa0qHZiH29oz","colab_type":"code","colab":{}},"source":["# keras-retinanet 다운로드 \n","%cd /content/DLCV/Detection/retina\n","!rm -rf /content/DLCV/Detection/retina/keras-retinanet\n","# fizyr keras-retinanet이 현재 keras 2.4 로 마이그레이션 되면서 버그가 많아짐.\n","#  tensorflow 1.15와 호환되는 keras-retinanet 버전(v0.5.1) 다운로드를 https://github.com/chulminkw/keras-retinanet-tf115.git 에서 수행. \n","!git clone https://github.com/chulminkw/keras-retinanet-tf115.git keras-retinanet\n","\n","#  https://github.com/chulminkw/keras-retinanet-tf115.git에서 download받은 keras-retinanet 설치\n","%cd /content/DLCV/Detection/retina/keras-retinanet\n","!echo \"##### installing keras-retinanet\"\n","!pip install . --user\n","!python setup.py build_ext --inplace"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b91TonqgDTmI","colab_type":"code","colab":{}},"source":["# 아래 import로 keras-retinanet이 정상적으로 설치되어 있는지 확인. 특히 backbone 함수 체크. \n","import tensorflow as tf\n","\n","from keras_retinanet import models\n","from keras_retinanet.models import backbone\n","\n","b = backbone('resnet50')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dImUm56p62B2","colab_type":"text"},"source":["#### Raccoon 데이터 세트 download\n","* Racoon 데이터 세트를 git clone으로 복사합니다. git clone https://github.com/experiencor/raccoon_dataset.git\n","* 이미지와 annoation 디렉토리를 제외하고 모두 삭제합니다.(코랩 버전은 적용 필요 없음)"]},{"cell_type":"code","metadata":{"id":"MSRQs5As61MD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1598699737349,"user_tz":-540,"elapsed":4586,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"e11b378a-71ce-4e82-fd31-2ce63a375ed5"},"source":["# /content/DLCV/data 디렉토리에 raccoon_dataset을 다운로드함. \n","%cd /content/DLCV/data/\n","!git clone https://github.com/experiencor/raccoon_dataset.git\n","# raccoon_dataset을 raccoon으로 디렉토리 이름 변경하고 확인 \n","!mv raccoon_dataset raccoon\n","!ls -lia"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/DLCV/data\n","Cloning into 'raccoon_dataset'...\n","remote: Enumerating objects: 646, done.\u001b[K\n","remote: Total 646 (delta 0), reused 0 (delta 0), pack-reused 646\u001b[K\n","Receiving objects: 100% (646/646), 48.00 MiB | 30.25 MiB/s, done.\n","Resolving deltas: 100% (412/412), done.\n","total 28\n","3291209 drwxr-xr-x 7 root root 4096 Aug 29 11:15 .\n","3291129 drwxr-xr-x 7 root root 4096 Aug 29 11:13 ..\n","3291210 drwxr-xr-x 2 root root 4096 Aug 29 11:13 image\n","3291223 drwxr-xr-x 2 root root 4096 Aug 29 11:13 output\n","3291407 drwxr-xr-x 7 root root 4096 Aug 29 11:15 raccoon\n","3291225 drwxr-xr-x 2 root root 4096 Aug 29 11:13 util\n","3291232 drwxr-xr-x 2 root root 4096 Aug 29 11:13 video\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1Mo7FNVI0knO","colab_type":"text"},"source":["#### Raccoon 데이터 세트의 image와 annotation 디렉토리 설정"]},{"cell_type":"code","metadata":{"id":"Loz_1CHU0knR","colab_type":"code","colab":{}},"source":["import os\n","from pathlib import Path\n","\n","#HOME_DIR = str(Path.home())\n","# 코랩 버전은 HOME_DIR을 /content 로 설정합니다.\n","HOME_DIR = '/content'\n","\n","ANNO_DIR = os.path.join(HOME_DIR, 'DLCV/data/raccoon/annotations')\n","IMAGE_DIR = os.path.join(HOME_DIR, 'DLCV/data/raccoon/images')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4htD2Sv0knc","colab_type":"text"},"source":["#### keras-retina 패키지의 사용 가능한 데이터 포맷\n","* keras-retina 패키지는 VOC, COCO, Open Image, 그리고 csv 형태의 데이터포맷을 모두 사용 가능 \n","* 하지만 VOC,COCO, OpenImage 모두 경연대회에서 사용된 디렉토리 구조가 필요함.   \n","* Raccoon Dataset가 VOC와 비슷한 포맷이지만 정확하게 VOC 디렉토리 구조를 가지고 있지는 않으므로 간편하게 csv 형태의 데이터 포맷을 활용하여 데이터 입력 적용\n","* csv 형태의 annotation과 class mapping format이 필요. annotation은 아래와 같이 표현 가능하며 하나의 오브젝트당 한 라인에서 comma로 정보를 분리함. 만일 하나의 이미지 파일에서 두개 이상의 오브젝트가 있다면 두개 이상의 라인으로 정보 표시.   \n","/data/imgs/img_001.jpg,837,346,981,456,cow  \n","/data/imgs/img_002.jpg,215,312,279,391,cat  \n","/data/imgs/img_002.jpg,22,5,89,84,bird  \n","/data/imgs/img_003.jpg,,,,,  "]},{"cell_type":"markdown","metadata":{"id":"b5lGOAMv0knf","colab_type":"text"},"source":["#### 학습과 검증 데이터 세트를 위한 별도의 annotation파일 생성. \n","* keras-retina패키지는 validation annotation파일을 이용하여 학습 시 evaluation 수행 가능\n","* 아래는 80%의 xml파일을 train csv로, 나머지 20% xml파일은 valid csv 로 생성"]},{"cell_type":"code","metadata":{"id":"GJGeBY0b0kng","colab_type":"code","colab":{}},"source":["import glob\n","import numpy as np\n","\n","def get_train_valid_indexes(anno_path, valid_size ):\n","    np.random.seed(0)\n","    \n","    xml_files = [xml_file for xml_file in glob.glob(os.path.join(anno_path, '*.xml'))]\n","    xml_files = np.array(xml_files)\n","    total_cnt = xml_files.shape[0]\n","    valid_cnt = int(total_cnt * valid_size)\n","    \n","    total_indexes = np.arange(0, total_cnt)\n","    valid_indexes = np.random.choice(total_cnt, valid_cnt, replace=False)\n","    train_indexes = total_indexes[~np.isin(total_indexes, valid_indexes)]\n","    \n","    return train_indexes, valid_indexes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yG2v_36f0kno","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1598699746689,"user_tz":-540,"elapsed":890,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"f25adaeb-0b1c-4b84-e942-8876964b759d"},"source":["train_indexes, valid_indexes = get_train_valid_indexes(ANNO_DIR, 0.2)\n","train_indexes.shape, valid_indexes.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((160,), (40,))"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"5QM2J9Sb0knv","colab_type":"text"},"source":["#### csv annotation 데이터 파일을 만들기 위한 함수 생성. \n","* 인자로 annotation 디렉토리명, csv형태로 만들어질 파일명을 주면 생성파일명으로 csv 형태의 annotation 데이터 파일 생성.  "]},{"cell_type":"code","metadata":{"id":"c5Rp3xdK0knx","colab_type":"code","colab":{}},"source":["import glob\n","import xml.etree.ElementTree as ET\n","\n","def xml_to_csv_sampling(path, output_filename, sample_index):\n","    xml_list = np.array([xml_file for xml_file in glob.glob(path + '/*.xml')])\n","    xml_list = xml_list[sample_index]\n","    # xml 확장자를 가진 모든 파일의 절대 경로로 xml_file할당. \n","    with open(output_filename, \"w\") as train_csv_file:\n","        for xml_file in xml_list:\n","            # xml 파일을 parsing하여 XML Element형태의 Element Tree를 생성하여 object 정보를 추출. \n","            tree = ET.parse(xml_file)\n","            root = tree.getroot()\n","            # 파일내에 있는 모든 object Element를 찾음. \n","            full_image_name = os.path.join(IMAGE_DIR, root.find('filename').text)\n","            value_str_list = ' '\n","            for obj in root.findall('object'):\n","                xmlbox = obj.find('bndbox')\n","                x1 = int(xmlbox.find('xmin').text)\n","                y1 = int(xmlbox.find('ymin').text)\n","                x2 = int(xmlbox.find('xmax').text)\n","                y2 = int(xmlbox.find('ymax').text)\n","                # 단 하나의 \n","                class_name='raccoon'\n","                value_str = ('{0},{1},{2},{3},{4},{5}').format(full_image_name,x1, y1, x2, y2, class_name)\n","                # object별 정보를 tuple형태로 object_list에 저장. \n","                train_csv_file.write(value_str+'\\n')\n","        # xml file 찾는 for loop 종료 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ladjy7WT0kn3","colab_type":"code","colab":{}},"source":["train_indexes, valid_indexes = get_train_valid_indexes(ANNO_DIR, 0.2)\n","xml_to_csv_sampling(ANNO_DIR, os.path.join(ANNO_DIR,'raccoon_anno_retina_train.csv'), train_indexes)\n","xml_to_csv_sampling(ANNO_DIR, os.path.join(ANNO_DIR,'raccoon_anno_retina_valid.csv'), valid_indexes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uREJ4dTx9hCD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1598699752083,"user_tz":-540,"elapsed":726,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"2c6dd30c-1dfc-43a9-d0ca-ae53d5f84a5e"},"source":["# colab 버전은 아래 명령어로 raccoon_class.txt 를 수정합니다. \n","default_dir = '/content/DLCV'\n","classes_path = os.path.join(default_dir, 'data/raccoon/annotations/raccoon_class.txt')\n","with open(classes_path, \"w\") as f:\n","    f.write(\"raccoon, 0\")\n","\n","# colab 버전은 raccoon_class.txt에 제대로 기재되었나 확인. \n","!cat /content/DLCV/data/raccoon/annotations/raccoon_class.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["raccoon, 0"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oNH0QRCMDkYU","colab_type":"text"},"source":["#### keras-retinanet으로 pretrained된 coco 모델 다운로드하고 해당 모델을 로드\n","* 앞 예제에서 pretrained 모델을 생성했지만 코렙 버전은 재 생성해야 함. \n","* 코렙 버전은 /content/DLCV/Detection/retina/keras-retinanet/snapshots 디렉토리 밑에 pretrained 모델을 download"]},{"cell_type":"code","metadata":{"id":"zayxNRwkDbAp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1598699757148,"user_tz":-540,"elapsed":3128,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"9a4c0aa3-2ba6-4fc7-c26f-4eb40cd08c13"},"source":["# 아래 모델은 https://github.com/fizyr/keras-retinanet/releases 에서 download 받을 수 있음. \n","# 해당 모델 h5 파일을 snapshot 디렉토리에 저장 후 retina model의 load_model()을 이용하여 모델 로딩.\n","%cd  /content/DLCV/Detection/retina/keras-retinanet/snapshots\n","!wget https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5 "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/DLCV/Detection/retina/keras-retinanet/snapshots\n","--2020-08-29 11:15:54--  https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/100249425/b7184a80-9350-11e9-9cc2-454f5c616394?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200829T111554Z&X-Amz-Expires=300&X-Amz-Signature=4135b9c8dba94f7131d24faf902ab61c15e45bdc896ecfa4829247fca37fa76a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.1.0.h5&response-content-type=application%2Foctet-stream [following]\n","--2020-08-29 11:15:54--  https://github-production-release-asset-2e65be.s3.amazonaws.com/100249425/b7184a80-9350-11e9-9cc2-454f5c616394?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200829T111554Z&X-Amz-Expires=300&X-Amz-Signature=4135b9c8dba94f7131d24faf902ab61c15e45bdc896ecfa4829247fca37fa76a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.1.0.h5&response-content-type=application%2Foctet-stream\n","Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.138.12\n","Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.138.12|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 152662144 (146M) [application/octet-stream]\n","Saving to: ‘resnet50_coco_best_v2.1.0.h5’\n","\n","resnet50_coco_best_ 100%[===================>] 145.59M  67.7MB/s    in 2.2s    \n","\n","2020-08-29 11:15:56 (67.7 MB/s) - ‘resnet50_coco_best_v2.1.0.h5’ saved [152662144/152662144]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xRYiFEE50kn-","colab_type":"text"},"source":["#### keras_retinanet/bin/train.py를 이용하여 학습 수행. \n","* keras-retinanet 패키지는 학습시간이 비교적 오래 필요. \n","* 특히 batch-size 가 크게 설정하기 어려움. 2이상 설정 시 메모리를 과다 사용으로 segmentation fault 오류 발생.\n","* Shell에서 export TF_CUDNN_USE_AUTOTUNE=0  설정하면 batch-size를 2로 늘릴 수 있으나 큰 학습 시간 단축은 기대하기 어려움. \n","* Raccoon 데이터 세트는 steps=200, epochs=20 정도면 충분한 학습이 가능. \n","* 학습 시 epoch가 완료될 때마다 snapshots 디렉토리에 모델들을 계속 생성하여 저장. \n","* train.py는 많은 환경 변수를 명령 인자로 입력해야함. 명령 인자로 입력하지 않을 경우 Default 환경 변수값으로 입력됨. Default 환경 변수값에 대한 이해 필요. "]},{"cell_type":"code","metadata":{"id":"nMIpTnye0kn_","colab_type":"code","colab":{}},"source":["# 아래는 shell에서 수행해야 합니다. 코랩 버전은 절대 경로로 디렉토리 수정. \n","# 코랩 버전은 학습에 상대적으로 시간이 더 많이 걸리므로 epochs는 10 정도로 수정하여 테스트 \n","!chmod +x /content/DLCV/Detection/retina/keras-retinanet/keras_retinanet/bin/train.py\n","!/content/DLCV/Detection/retina/keras-retinanet/keras_retinanet/bin/train.py --epochs=10 --steps=200 \\\n","  csv /content/DLCV/data/raccoon/annotations/raccoon_anno_retina_train.csv \\\n","      /content/DLCV/data/raccoon/annotations/raccoon_class.txt \\\n","      --val-annotations=/content/DLCV/data/raccoon/annotations/raccoon_anno_retina_valid.csv \\\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vV75TSr80koF","colab_type":"text"},"source":["#### train.py의 여러 모듈을 직접 import하여 학습 수행. \n","* train.py의 여러 모듈을 직접 import하여 customization으로 학습을 수행하는 것이 더 직관적이고 빠른 학습 시간 보장.\n","* keras-retinanet으로 학습 시 어떻게 내부 모듈이 동작하는지 더 명확히 알 수 있음. \n","* 환경 파라미터를 훨씬 편하게 조정 가능"]},{"cell_type":"code","metadata":{"id":"JRxH9g6qxDd5","colab_type":"code","colab":{}},"source":["import cv2\n","from os import listdir, walk\n","import math\n","import tensorflow as tf\n","import numpy as np\n","from os.path import join\n","from keras_retinanet.bin.train import create_generators,create_models,create_callbacks\n","from keras_retinanet.models import backbone,load_model,convert_model\n","from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n","from keras_retinanet.utils.visualization import draw_boxes\n","\n","#from imgaug import augmenters as iaa\n","\n","tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n","#tf.random.set_seed(31) \n","np.random.seed(17)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dlgIgrwD0koL","colab_type":"text"},"source":["#### 환경 파라미터 설정. "]},{"cell_type":"code","metadata":{"id":"_nKxa9vv0koN","colab_type":"code","colab":{}},"source":["\n","from keras_retinanet.bin.train import create_generators,create_models,create_callbacks\n","from keras_retinanet.models import backbone,load_model,convert_model\n","from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n","from keras_retinanet.utils.visualization import draw_boxes\n","\n","b = backbone('resnet50')\n","files = os.listdir(ANNO_DIR)\n","train_file_cnt = train_indexes.shape[0]\n","\n","class args:\n","    batch_size = 4\n","    config = None\n","    random_transform = True # Image augmentation\n","    annotations = os.path.join(ANNO_DIR, 'raccoon_anno_retina_train.csv')\n","    val_annotations = os.path.join(ANNO_DIR, 'raccoon_anno_retina_valid.csv')\n","    classes = os.path.join(ANNO_DIR, 'raccoon_class.txt')\n","    image_min_side = 800\n","    image_max_side = 1333\n","    no_resize=None\n","    dataset_type = 'csv'\n","    tensorboard_dir = ''\n","    evaluation = True\n","    snapshots = True\n","    # 코랩 버전은 아래를 절대 경로로 변경. \n","    snapshot_path = '/content/DLCV/Detection/retina/keras-retinanet/snapshots'\n","    backbone = 'resnet50'\n","    # 코랩 버전은 학습 속도가 느림, 기다리기 어려울 경우 아래 epochs를 20에서 10으로 낮춤. \n","    epochs = 20\n","    steps = train_file_cnt//(batch_size)\n","    weighted_average = True\n","    # 코랩 버전 keras-retinanet upgrade에 따른  신규 추가\n","    #reduce_lr_patience = 2\n","    #reduce_lr_factor = 0.1\n","    #  코랩 버전 keras-retinanet upgrade에 따른 신규 추가 2020.08.29\n","    #group_method='ratio'\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Sx1uNv50koT","colab_type":"text"},"source":["#### 학습과 검증을 위한 generator 생성."]},{"cell_type":"code","metadata":{"id":"zjg6ykTv0koU","colab_type":"code","colab":{}},"source":["train_gen,valid_gen = create_generators(args,b.preprocess_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjgSrmcG0koZ","colab_type":"text"},"source":["#### backend CNN과 기타 환경 설정하여 기본 모델 생성"]},{"cell_type":"code","metadata":{"id":"bU7ns5hv0kob","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1598699785655,"user_tz":-540,"elapsed":3584,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"cac6aea3-121a-4f91-d5c0-a2ca6aab988d"},"source":["model, training_model, prediction_model = create_models(\n","            backbone_retinanet=b.retinanet,\n","            num_classes=train_gen.num_classes(),\n","            weights=None,\n","            multi_gpu=False,\n","            freeze_backbone=True,\n","            lr=1e-3,\n","            config=args.config\n","        )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tracking <tf.Variable 'Variable_5:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_6:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_7:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_8:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_9:0' shape=(9, 4) dtype=float32> anchors\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ee0o21b50koi","colab_type":"text"},"source":["#### Checkpoint, ReduceLROnPlateur와 같은 callback 기능 생성. "]},{"cell_type":"code","metadata":{"id":"C5T3P1NN0koi","colab_type":"code","colab":{}},"source":["callbacks = create_callbacks(\n","    model,\n","    training_model,\n","    prediction_model,\n","    valid_gen,\n","    args,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B5Yfp2J20koq","colab_type":"text"},"source":["#### 학습 모델에 coco로 pretrained된 weight를 최초 weight로 설정"]},{"cell_type":"code","metadata":{"id":"YQ6hZL-J0koq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"status":"ok","timestamp":1598699796391,"user_tz":-540,"elapsed":4435,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"d3438d27-2a34-48b2-bb4b-19730449c6f4"},"source":["# 코랩 버전은 절대 경로로 수정. \n","default_retina_dir='/content/DLCV/Detection/retina'\n","training_model.load_weights(os.path.join(default_retina_dir, 'keras-retinanet/snapshots/resnet50_coco_best_v2.1.0.h5'),skip_mismatch=True,by_name=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1316: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 9) vs (720, 256, 3, 3)).\n","  weight_values[i].shape))\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1316: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((9,) vs (720,)).\n","  weight_values[i].shape))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lkumCkob0kov","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598701808419,"user_tz":-540,"elapsed":2011208,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"020e54d1-f4e5-454d-ec8c-6636372458db"},"source":["training_model.fit_generator(generator=train_gen,\n","        steps_per_epoch=args.steps,\n","        epochs=args.epochs,\n","        verbose=1,\n","        validation_data=valid_gen,                     \n","        callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Epoch 1/20\n","40/40 [==============================] - 214s 5s/step - loss: 1.5943 - regression_loss: 1.2661 - classification_loss: 0.3282 - val_loss: 1.3987 - val_regression_loss: 1.2630 - val_classification_loss: 0.2892\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:33 Time:  0:00:33\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9166\n","mAP: 0.9166\n","\n","Epoch 00001: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_01.h5\n","Epoch 2/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.5368 - regression_loss: 1.3437 - classification_loss: 0.1931 - val_loss: 1.9803 - val_regression_loss: 1.4823 - val_classification_loss: 0.1626\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9716\n","mAP: 0.9716\n","\n","Epoch 00002: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_02.h5\n","Epoch 3/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.6402 - regression_loss: 1.4250 - classification_loss: 0.2152 - val_loss: 1.5613 - val_regression_loss: 1.3240 - val_classification_loss: 0.2078\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9403\n","mAP: 0.9403\n","\n","Epoch 00003: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_03.h5\n","Epoch 4/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.4360 - regression_loss: 1.2407 - classification_loss: 0.1953 - val_loss: 1.2700 - val_regression_loss: 1.2857 - val_classification_loss: 0.2171\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9577\n","mAP: 0.9577\n","\n","Epoch 00004: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_04.h5\n","Epoch 5/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.4366 - regression_loss: 1.2656 - classification_loss: 0.1710 - val_loss: 1.7262 - val_regression_loss: 1.2410 - val_classification_loss: 0.1705\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9221\n","mAP: 0.9221\n","\n","Epoch 00005: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_05.h5\n","Epoch 6/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.4025 - regression_loss: 1.2302 - classification_loss: 0.1723 - val_loss: 1.6084 - val_regression_loss: 1.2616 - val_classification_loss: 0.1973\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9121\n","mAP: 0.9121\n","\n","Epoch 00006: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_06.h5\n","Epoch 7/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.4466 - regression_loss: 1.2850 - classification_loss: 0.1615 - val_loss: 1.6175 - val_regression_loss: 1.4682 - val_classification_loss: 0.2541\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.8847\n","mAP: 0.8847\n","\n","Epoch 00007: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_07.h5\n","Epoch 8/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.3098 - regression_loss: 1.1560 - classification_loss: 0.1539 - val_loss: 1.6196 - val_regression_loss: 1.2856 - val_classification_loss: 0.2246\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9318\n","mAP: 0.9318\n","\n","Epoch 00008: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_08.h5\n","Epoch 9/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.3096 - regression_loss: 1.1596 - classification_loss: 0.1499 - val_loss: 1.8051 - val_regression_loss: 1.3280 - val_classification_loss: 0.2934\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9281\n","mAP: 0.9281\n","\n","Epoch 00009: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_09.h5\n","Epoch 10/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.3492 - regression_loss: 1.1881 - classification_loss: 0.1611 - val_loss: 1.7372 - val_regression_loss: 1.4708 - val_classification_loss: 0.2053\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9208\n","mAP: 0.9208\n","\n","Epoch 00010: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_10.h5\n","Epoch 11/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.2390 - regression_loss: 1.0966 - classification_loss: 0.1424 - val_loss: 1.7697 - val_regression_loss: 1.5586 - val_classification_loss: 0.2376\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9548\n","mAP: 0.9548\n","\n","Epoch 00011: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_11.h5\n","Epoch 12/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.2085 - regression_loss: 1.0814 - classification_loss: 0.1271 - val_loss: 1.6237 - val_regression_loss: 1.2620 - val_classification_loss: 0.1986\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9328\n","mAP: 0.9328\n","\n","Epoch 00012: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_12.h5\n","Epoch 13/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.2187 - regression_loss: 1.1158 - classification_loss: 0.1029 - val_loss: 1.5262 - val_regression_loss: 1.3094 - val_classification_loss: 0.1798\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9625\n","mAP: 0.9625\n","\n","Epoch 00013: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_13.h5\n","Epoch 14/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.1819 - regression_loss: 1.0709 - classification_loss: 0.1110 - val_loss: 1.5501 - val_regression_loss: 1.7062 - val_classification_loss: 0.2185\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9124\n","mAP: 0.9124\n","\n","Epoch 00014: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_14.h5\n","Epoch 15/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.1884 - regression_loss: 1.0701 - classification_loss: 0.1184 - val_loss: 1.3246 - val_regression_loss: 1.3866 - val_classification_loss: 0.1888\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9328\n","mAP: 0.9328\n","\n","Epoch 00015: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_15.h5\n","Epoch 16/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.1584 - regression_loss: 1.0448 - classification_loss: 0.1136 - val_loss: 1.2680 - val_regression_loss: 1.3705 - val_classification_loss: 0.2206\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9460\n","mAP: 0.9460\n","\n","Epoch 00016: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_16.h5\n","Epoch 17/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.1519 - regression_loss: 1.0490 - classification_loss: 0.1029 - val_loss: 2.1712 - val_regression_loss: 1.5955 - val_classification_loss: 0.2001\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.8817\n","mAP: 0.8817\n","\n","Epoch 00017: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_17.h5\n","Epoch 18/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.1397 - regression_loss: 1.0532 - classification_loss: 0.0865 - val_loss: 1.6841 - val_regression_loss: 1.2892 - val_classification_loss: 0.1650\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9535\n","mAP: 0.9535\n","\n","Epoch 00018: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_18.h5\n","Epoch 19/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.1126 - regression_loss: 1.0301 - classification_loss: 0.0825 - val_loss: 1.5134 - val_regression_loss: 1.2568 - val_classification_loss: 0.1409\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9548\n","mAP: 0.9548\n","\n","Epoch 00019: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_19.h5\n","Epoch 20/20\n","40/40 [==============================] - 79s 2s/step - loss: 1.1559 - regression_loss: 1.0556 - classification_loss: 0.1003 - val_loss: 1.2557 - val_regression_loss: 1.3249 - val_classification_loss: 0.1962\n"],"name":"stdout"},{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"},{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9065\n","mAP: 0.9065\n","\n","Epoch 00020: saving model to /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_20.h5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fca6e45fe48>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"i8a7qs3l0ko2","colab_type":"text"},"source":["### 학습 모델 기반 Object Detection 및 Detection 성능 평가(Evaluation)"]},{"cell_type":"markdown","metadata":{"id":"BZ5MP-v20ko2","colab_type":"text"},"source":["#### 학습 모델을 Inference 모델로 변환\n","*  keras_retinanet/bin/convert_model.py를 이용하여 snapshots 디렉토리에 가장 마지막에 만들어진 학습 모델(가장 손실율이 적은)을 infererence용 모델로 변환"]},{"cell_type":"code","metadata":{"id":"ZfFOgwioiLFe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1598702217031,"user_tz":-540,"elapsed":767,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"a802e971-e3eb-4f94-f98c-9bb735c16679"},"source":["!ls /content/DLCV/Detection/retina/keras-retinanet/snapshots"],"execution_count":null,"outputs":[{"output_type":"stream","text":["resnet50_coco_best_v2.1.0.h5  resnet50_csv_08.h5  resnet50_csv_16.h5\n","resnet50_csv_01.h5\t      resnet50_csv_09.h5  resnet50_csv_17.h5\n","resnet50_csv_02.h5\t      resnet50_csv_10.h5  resnet50_csv_18.h5\n","resnet50_csv_03.h5\t      resnet50_csv_11.h5  resnet50_csv_19.h5\n","resnet50_csv_04.h5\t      resnet50_csv_12.h5  resnet50_csv_20.h5\n","resnet50_csv_05.h5\t      resnet50_csv_13.h5  test01\n","resnet50_csv_06.h5\t      resnet50_csv_14.h5\n","resnet50_csv_07.h5\t      resnet50_csv_15.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M3C5wi1qOHTQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":753},"executionInfo":{"status":"ok","timestamp":1598702254920,"user_tz":-540,"elapsed":31551,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"b4d5eda5-7b6a-4876-ab61-26cc5c3eaf5b"},"source":["# 가장 마지막에 만들어진 학습 모델을 변환합니다. \n","!chmod +x /content/DLCV/Detection/retina/keras-retinanet/keras_retinanet/bin/convert_model.py\n","!/content/DLCV/Detection/retina/keras-retinanet/keras_retinanet/bin/convert_model.py /content/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_20.h5 \\\n","/content/DLCV/Detection/retina/keras-retinanet/snapshots/raccoon_inference.h5"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","WARNING:tensorflow:From /content/DLCV/Detection/retina/keras-retinanet/keras_retinanet/bin/convert_model.py:40: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /content/DLCV/Detection/retina/keras-retinanet/keras_retinanet/bin/convert_model.py:42: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2020-08-29 11:57:06.997453: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2020-08-29 11:57:07.002132: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2020-08-29 11:57:07.002390: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x32acbc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-08-29 11:57:07.002422: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-08-29 11:57:07.006910: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-08-29 11:57:07.010094: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2020-08-29 11:57:07.010167: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 17b48cd94f9c\n","2020-08-29 11:57:07.010184: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 17b48cd94f9c\n","2020-08-29 11:57:07.010265: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.67.0\n","2020-08-29 11:57:07.010317: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.67.0\n","2020-08-29 11:57:07.010363: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.67.0\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /content/DLCV/Detection/retina/keras-retinanet/keras_retinanet/bin/../../keras_retinanet/backend/tensorflow_backend.py:68: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n","\n","2020-08-29 11:57:11.782036: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 18874368 exceeds 10% of system memory.\n","WARNING:tensorflow:From /content/DLCV/Detection/retina/keras-retinanet/keras_retinanet/bin/../../keras_retinanet/backend/tensorflow_backend.py:104: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2020-08-29 11:57:15.551268: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 18874368 exceeds 10% of system memory.\n","2020-08-29 11:57:15.561522: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 18874368 exceeds 10% of system memory.\n","2020-08-29 11:57:15.586890: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 18874368 exceeds 10% of system memory.\n","2020-08-29 11:57:15.633398: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 18874368 exceeds 10% of system memory.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","tracking <tf.Variable 'Variable:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_1:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_2:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_3:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_4:0' shape=(9, 4) dtype=float32> anchors\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"98-shzpByFXx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1598702259171,"user_tz":-540,"elapsed":761,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"a84b9cf6-d509-405d-9539-391e296ae33a"},"source":["!ls /content/DLCV/Detection/retina/keras-retinanet/snapshots/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["raccoon_inference.h5\t      resnet50_csv_07.h5  resnet50_csv_15.h5\n","resnet50_coco_best_v2.1.0.h5  resnet50_csv_08.h5  resnet50_csv_16.h5\n","resnet50_csv_01.h5\t      resnet50_csv_09.h5  resnet50_csv_17.h5\n","resnet50_csv_02.h5\t      resnet50_csv_10.h5  resnet50_csv_18.h5\n","resnet50_csv_03.h5\t      resnet50_csv_11.h5  resnet50_csv_19.h5\n","resnet50_csv_04.h5\t      resnet50_csv_12.h5  resnet50_csv_20.h5\n","resnet50_csv_05.h5\t      resnet50_csv_13.h5  test01\n","resnet50_csv_06.h5\t      resnet50_csv_14.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DiSvPzoE0ko3","colab_type":"code","colab":{}},"source":["import sys\n","import os\n","ROOT_DIR = os.path.abspath(\".\")\n","sys.path.append(ROOT_DIR)\n","\n","#!./keras-retinanet/keras_retinanet/bin/convert_model.py ~/DLCV/Detection/retina/keras-retinanet/snapshots/resnet50_csv_20.h5 \\\n","#~/DLCV/Detection/retina/keras-retinanet/snapshots/raccoon_inference.h5\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A2CoiDnl0ko7","colab_type":"text"},"source":["#### 변환된 inference용 모델인 raccoon_inference.h5 파일을 로드하여 이미지 Detection 수행"]},{"cell_type":"code","metadata":{"id":"CMhDqPmt0ko8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1598702396824,"user_tz":-540,"elapsed":772,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"dbdb3c75-eefe-42a0-d991-a83ee76d639e"},"source":["# show images inline\n","%matplotlib inline\n","\n","# automatically reload modules when they have changed\n","%load_ext autoreload\n","%autoreload 2\n","\n","# import keras\n","import keras\n","\n","# import miscellaneous modules\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","import numpy as np\n","import time\n","\n","\n","from keras_retinanet import models\n","from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n","from keras_retinanet.utils.visualization import draw_box, draw_caption\n","from keras_retinanet.utils.colors import label_color\n","#from keras_retinanet.utils.gpu import setup_gpu\n","\n","# use this to change which GPU to use\n","gpu = 0\n","\n","# set the modified tf session as backend in keras\n","#setup_gpu(gpu)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MeBDvpEz0kpB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":188},"executionInfo":{"status":"ok","timestamp":1598702409327,"user_tz":-540,"elapsed":6749,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"b001f60a-99d2-43e0-9f9f-419133433e56"},"source":["import os\n","import sys\n","\n","# 코랩 버전은 절대 경로 추가. \n","#ROOT_DIR = os.path.abspath(\".\")\n","#sys.path.append(ROOT_DIR)\n","ROOT_DIR = '/content/DLCV/Detection/retina'\n","\n","model_path = os.path.join(ROOT_DIR, 'keras-retinanet/snapshots/raccoon_inference.h5')\n","\n","print(model_path)\n","# load retinanet model\n","raccoon_retina_model = models.load_model(model_path, backbone_name='resnet50')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/DLCV/Detection/retina/keras-retinanet/snapshots/raccoon_inference.h5\n","tracking <tf.Variable 'Variable_10:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_11:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_12:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_13:0' shape=(9, 4) dtype=float32> anchors\n","tracking <tf.Variable 'Variable_14:0' shape=(9, 4) dtype=float32> anchors\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n","  warnings.warn('No training configuration found in save file: '\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"zAePFLze0kpF","colab_type":"text"},"source":["#### 이미지 detect를 위한 함수 생성. \n","* inference를 수행하기 전에 이미지 scaling 및 크기를 재 조정할 수 있도록 preprocess_image()와 resize_image() 제공. \n","* keras-retinanet은 이미지에 bounding box를 편리하게 그릴 수 있는 API제공. draw_box(), draw_caption(), label_color() 제공"]},{"cell_type":"code","metadata":{"id":"St3MjnRU0kpH","colab_type":"code","colab":{}},"source":["import cv2\n","from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n","from keras_retinanet.utils.visualization import draw_box, draw_caption\n","from keras_retinanet.utils.colors import label_color\n","\n","labels_to_names_seq = {0:'Raccoon'}\n","\n","def get_detected_image_retina(model, img_array, use_copied_array, is_print=True):\n","    \n","    # copy to draw on\n","    draw_img = None\n","    if use_copied_array:\n","        draw_img = img_array.copy()\n","    else:\n","        draw_img = img_array\n","    \n","    img_array = preprocess_image(img_array)\n","    img_array, scale = resize_image(img_array)\n","    \n","    # process image\n","    start = time.time()\n","    boxes, scores, labels = model.predict_on_batch(np.expand_dims(img_array, axis=0))\n","    if is_print:\n","        print(\"object detection 처리 시간: \", round(time.time() - start,5))\n","    \n","    # correct for image scale\n","    boxes /= scale\n","\n","    # visualize detections\n","    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n","        # scores are sorted so we can break\n","        if score < 0.5:\n","            break\n","\n","        color = label_color(label)\n","\n","        b = box.astype(int)\n","        draw_box(draw_img, b, color=color)\n","\n","        caption = \"{} {:.3f}\".format(labels_to_names_seq[label], score)\n","        draw_caption(draw_img, b, caption)\n","    \n","    if is_print:\n","        print(\"이미지 processing 시간: \", round(time.time() - start,5))\n","    \n","    return draw_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0Ox9jIo0kpL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"18FweYHtdkfVEwZqD17G_pQB8oTaWv-wD"},"executionInfo":{"status":"ok","timestamp":1598702423264,"user_tz":-540,"elapsed":4977,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"2f42c308-ff32-4a48-c618-36de0e73ed4d"},"source":["import os\n","from pathlib import Path\n","\n","#HOME_DIR = str(Path.home())\n","# 코랩 버전 수정\n","HOME_DIR = '/content'\n","ANNO_DIR = os.path.join(HOME_DIR, 'DLCV/data/raccoon/annotations')\n","IMAGE_DIR = os.path.join(HOME_DIR, 'DLCV/data/raccoon/images')\n","\n","img_array  = cv2.imread(os.path.join(IMAGE_DIR, 'raccoon-22.jpg'))\n","draw_img_array = img_array.copy()\n","draw_img_array = cv2.cvtColor(draw_img_array, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(15, 15))\n","plt.axis('off')\n","plt.imshow(draw_img_array)\n","plt.show()\n","\n","detected_image = get_detected_image_retina(raccoon_retina_model, img_array, use_copied_array=True, is_print=True)\n","img_rgb = cv2.cvtColor(detected_image, cv2.COLOR_BGR2RGB)\n","plt.figure(figsize=(15, 15))\n","plt.axis('off')\n","plt.imshow(img_rgb)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"nKbsv8ru0kpP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1zLI8oISNSzArB7Owel-naXj5jAkgxSD3"},"executionInfo":{"status":"ok","timestamp":1598702437215,"user_tz":-540,"elapsed":15589,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"0d8adfda-b702-406e-d109-ec78f5633e0d"},"source":["import numpy as np\n","np.random.seed(0)\n","\n","# 모든 이미지 파일중에서 임의의 16개 파일만 설정. \n","all_image_files = glob.glob(IMAGE_DIR + '/*.jpg')\n","all_image_files = np.array(all_image_files)\n","file_cnt = all_image_files.shape[0]\n","show_cnt = 16\n","\n","show_indexes = np.random.choice(file_cnt, show_cnt)\n","show_files = all_image_files[show_indexes]\n","print(show_files)\n","fig, axs = plt.subplots(figsize=(24,24) , ncols=4 , nrows=4)\n","\n","for i , filename in enumerate(show_files):\n","    print(filename)\n","    row = int(i/4)\n","    col = i%4\n","    img_array = cv2.imread(os.path.join(IMAGE_DIR, filename))\n","    detected_image = get_detected_image_retina(raccoon_retina_model,img_array, use_copied_array=True, is_print=True)\n","    img_rgb = cv2.cvtColor(detected_image, cv2.COLOR_BGR2RGB)\n","    axs[row][col].imshow(img_rgb)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"KBWNFLMz0kpU","colab_type":"text"},"source":["#### video에 object detection을 수행\n","* get_detected_image()와 유사한 함수를 생성. 인자로 image array와 retina 모델을 입력, 개별 frame별로 object Detection 수행. "]},{"cell_type":"code","metadata":{"id":"DFU1cKmf0kpU","colab_type":"code","colab":{}},"source":["def detect_video_retina(model, input_path, output_path=\"\"):\n","    \n","    start = time.time()\n","    cap = cv2.VideoCapture(input_path)\n","    \n","    codec = cv2.VideoWriter_fourcc(*'XVID')\n","    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n","    vid_size= (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n","    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size)\n","    \n","    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    print('총 Frame 갯수:', frame_cnt)\n","    \n","    while True:\n","        hasFrame, image_frame = cap.read()\n","        if not hasFrame:\n","            print('프레임이 없거나 종료 되었습니다.')\n","            break\n","\n","        detected_image = get_detected_image_retina(model,image_frame, use_copied_array=False, is_print=True)\n","        vid_writer.write(detected_image)\n","    \n","    vid_writer.release()\n","    cap.release()\n","    print('### Video Detect 총 수행시간:', round(time.time()-start, 5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3Q0WgF30kpZ","colab_type":"code","colab":{}},"source":["# 코랩 버전 절대 경로로 수정\n","default_dir = '/content/DLCV'\n","detect_video_retina(raccoon_retina_model, input_path=os.path.join(default_dir, 'data/video/jack_and_raccoon.mp4'), \n","                    output_path=os.path.join(default_dir, 'data/output/jack_retina.avi'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37htcExK0kpg","colab_type":"code","colab":{}},"source":["# 코랩 버전은 GOOGLE DRIVE 로 UPLOAD 수행. \n","!gsutil cp ~/DLCV/data/output/jack_retina.avi gs://my_bucket_dlcv/data/output/jack_retina.avi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nWA-IQG80kpi","colab_type":"text"},"source":["#### Raccoon 데이터 세트 학습 모델의 Object Detection 성능 평가"]},{"cell_type":"code","metadata":{"id":"NYTtN3VX0kpk","colab_type":"code","colab":{}},"source":["from keras_retinanet.bin.evaluate import create_generator as eval_create_generator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6fCeDj-0kpp","colab_type":"code","colab":{}},"source":["import os\n","from pathlib import Path\n","\n","#HOME_DIR = str(Path.home())\n","# 코랩 버전 수정\n","HOME_DIR = '/content'\n","ANNO_DIR = os.path.join(HOME_DIR, 'DLCV/data/raccoon/annotations')\n","\n","class args:\n","    dataset_type='csv'\n","    score_threshold=0.05\n","    iou_threshold=0.5\n","    max_detections=100\n","    image_min_side=800\n","    image_max_side=1333\n","    config=None\n","    annotations=os.path.join(ANNO_DIR, 'raccoon_anno_retina_valid.csv')\n","    classes=os.path.join(ANNO_DIR, 'raccoon_class.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"esnlhlwI0kps","colab_type":"code","colab":{}},"source":["generator = eval_create_generator(args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X93enBxH0kpx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1598702835710,"user_tz":-540,"elapsed":11946,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"69d797b4-0523-4725-85c5-7c92efe2e54d"},"source":["from keras_retinanet.utils.eval import evaluate\n","\n","average_precisions = evaluate(\n","            generator,\n","            raccoon_retina_model,\n","            iou_threshold=args.iou_threshold,\n","            score_threshold=args.score_threshold,\n","            max_detections=args.max_detections\n","        )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running network: 100% (40 of 40) |#######| Elapsed Time: 0:00:11 Time:  0:00:11\n","Parsing annotations: 100% (40 of 40) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tvM6CF8B0kp1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1598702852244,"user_tz":-540,"elapsed":702,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"c7b9549a-7fb6-4407-e440-da28f7bb3d22"},"source":["# print evaluation\n","total_instances = []\n","precisions = []\n","for label, (average_precision, num_annotations) in average_precisions.items():\n","    print('{:.0f} instances of class'.format(num_annotations),\n","          generator.label_to_name(label), 'with average precision: {:.4f}'.format(average_precision))\n","    total_instances.append(num_annotations)\n","    precisions.append(average_precision)\n","\n","if sum(total_instances) == 0:\n","    print('No test instances found.')\n","\n","#print('Inference time for {:.0f} images: {:.4f}'.format(generator.size(), inference_time))\n","\n","print('mAP using the weighted average of precisions among classes: {:.4f}'.format(sum([a * b for a, b in zip(total_instances, precisions)]) / sum(total_instances)))\n","print('mAP: {:.4f}'.format(sum(precisions) / sum(x > 0 for x in total_instances)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["45 instances of class raccoon with average precision: 0.9065\n","mAP using the weighted average of precisions among classes: 0.9065\n","mAP: 0.9065\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"93dCHoVt0kp7","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}